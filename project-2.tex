\documentclass{article}
\author{Monnier Killian\and André Émilien}
\title{Project 2: Explainable Artificial Intelligence and Cybersecurity}
\begin{document}
\maketitle
\section{Introduction}
Neural networks with fine-tuned weights outperform ML algorithms but their results are difficult to interpret, and, one cannot manually define a rule in the prediction process.\par
This lack of interpretability prevents banks and other financial institutions from using them.\par
Explainable AI is a field of study that aims to address these issues.\par
Another advantage of XAI is when rules are defined it makes it impossible for training biases.\par
The use case we are dealing with in this project is the use of Intrusion Detection System (IDS) based on XAI in Security Information and Event Management (SIEM). A SIEM analyst should have explainations on detected threats to take important decisions such as shutting down the Information System (IS).
\section{State of the Art}
Current explainable IA methods are SHAP and BRCG. Protodash shows samples from training dataset which are similar to given sample, it shows similarities and differences between them, etc. For end user LIME, SHAP and CEM explain which features in the input instance are contributing in model's final decision and how model's decision can be changed by tweaking their values. [1]
\section{References}
[1] : Mane, Shraddha \& Rao, Dattaraj. (2021). Explaining Network Intrusion
Detection System Using Explainable AI Framework.
\end{document}
